{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from music21 import converter, instrument, note, chord\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \n",
    "    notes = []\n",
    "    \n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    for file in glob.glob(\"midi_songs/*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        #print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "        \n",
    "    return notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = get_notes() \n",
    "\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "vocab_size = len(note_to_int)\n",
    "\n",
    "seq_len = 100\n",
    "\n",
    "    \n",
    "in_seq_0 = []\n",
    "tar_seq_0 = []\n",
    "\n",
    "# create input sequences and the corresponding outputs\n",
    "for i in range(0, len(notes) - seq_len, 1):\n",
    "    sequence_in = notes[i:i + seq_len]\n",
    "    sequence_out = notes[i + seq_len]\n",
    "    \n",
    "    for j in range(0, seq_len, 1):\n",
    "        in_seq_0.append(note_to_int[sequence_in[j]])\n",
    "        \n",
    "    tar_seq_0.append(note_to_int[sequence_out])\n",
    "# save the input- and outputsseq to the file \n",
    "with open('notes_in', 'wb') as fp:\n",
    "    pickle.dump(in_seq_0, fp)\n",
    "    \n",
    "with open('notes_tar', 'wb') as fp:\n",
    "    pickle.dump(tar_seq_0, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, batch_size, hidden_dim, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim).cuda(),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim).cuda())\n",
    "    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        hidden = self.init_hidden()\n",
    "        \n",
    "        embeds = self.embeddings(inputs)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds.view(len(net_in_tensor), 1, -1), hidden)\n",
    "        \n",
    "        prediction = self.linear(lstm_out.view(len(net_in_tensor), -1))\n",
    "        pre_scores = F.log_softmax(prediction, dim=1)\n",
    "        return pre_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_size = vocab_size\n",
    "\n",
    "embedding_dim = 30\n",
    "batch_size = 1\n",
    "hidden_dim = 36\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LSTM(input_size, embedding_dim, batch_size, hidden_dim, vocab_size)\n",
    "model.cuda()\n",
    "\n",
    "model.zero_grad()\n",
    "model.hidden = model.init_hidden()\n",
    "\n",
    "lossfunction = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "\n",
    "Epochs = 500\n",
    "\n",
    "save_every = 50\n",
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0: Total Loss = 14191.2861328125, Avg. Time/Epoch = 8.89884386062622\n",
      "Epoch 10: Total Loss = 13174.861328125, Avg. Time/Epoch = 86.58851318359375\n",
      "Epoch 20: Total Loss = 12990.9951171875, Avg. Time/Epoch = 86.9698876619339\n",
      "Epoch 30: Total Loss = 13117.634765625, Avg. Time/Epoch = 82.17859997749329\n",
      "Epoch 40: Total Loss = 12895.453125, Avg. Time/Epoch = 83.4453234910965\n",
      "Epoch 50: Total Loss = 12946.40234375, Avg. Time/Epoch = 86.04122703075409\n",
      "Epoch 60: Total Loss = 12998.224609375, Avg. Time/Epoch = 81.96825749874115\n",
      "Epoch 70: Total Loss = 12941.822265625, Avg. Time/Epoch = 91.37112436294555\n",
      "Epoch 80: Total Loss = 13002.9228515625, Avg. Time/Epoch = 90.20939438343048\n",
      "Epoch 90: Total Loss = 12928.4443359375, Avg. Time/Epoch = 86.07905864715576\n",
      "Epoch 100: Total Loss = 13099.7568359375, Avg. Time/Epoch = 81.65858454704285\n",
      "Epoch 110: Total Loss = 12998.28125, Avg. Time/Epoch = 87.99343454837799\n",
      "Epoch 120: Total Loss = 13029.65234375, Avg. Time/Epoch = 81.33473708629609\n",
      "Epoch 130: Total Loss = 12966.708984375, Avg. Time/Epoch = 81.23301718235015\n",
      "Epoch 140: Total Loss = 12951.6884765625, Avg. Time/Epoch = 90.28254563808441\n",
      "Epoch 150: Total Loss = 13109.291015625, Avg. Time/Epoch = 96.72059156894684\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-98dd71cad0ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_seq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "start = time.time()\n",
    "total_Loss = []\n",
    "\n",
    "for e in range(Epochs):\n",
    "    \n",
    "    total_Loss.append(0)\n",
    "    counter = 0\n",
    "    with open ('notes_in', 'rb') as fp:\n",
    "        in_seq = pickle.load(fp)\n",
    "    \n",
    "    with open ('notes_tar', 'rb') as fp:\n",
    "        tar_seq = pickle.load(fp)\n",
    "    \n",
    "    while len(in_seq) != 0:\n",
    "        \n",
    "        if (len(in_seq) >= 100 and len(tar_seq) != 0):\n",
    "            counter += 1\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            net_in = in_seq[:100]\n",
    "            del in_seq[:100]\n",
    "            net_in_tensor= torch.tensor(net_in, dtype=torch.long).cuda()\n",
    "            net_out = model(net_in_tensor)\n",
    "            #get the last list of output array as output1\n",
    "            net_out1 = torch.narrow(net_out, 0, 99, 1)\n",
    "            target1 = torch.tensor([tar_seq.pop()], dtype=torch.long).cuda()\n",
    "            \n",
    "            if (len(in_seq) >= 100 and len(tar_seq) != 0):\n",
    "                counter += 1\n",
    "                optimizer.zero_grad()\n",
    "                net_in = in_seq[:100]\n",
    "                del in_seq[:100]\n",
    "                net_in_tensor= torch.tensor(net_in, dtype=torch.long).cuda()\n",
    "                net_out = model(net_in_tensor)\n",
    "                #get the last list of output array as output2\n",
    "                net_out2 = torch.narrow(net_out, 0, 99, 1)\n",
    "                target2 = torch.tensor([tar_seq.pop()], dtype=torch.long).cuda()\n",
    "                #change the dimention of the inputs of lossfunction from 1 to 2\n",
    "                output_seq = torch.cat((net_out1, net_out2), 0)\n",
    "                target_seq = torch.cat((target1, target2), 0)\n",
    "                loss = lossfunction(output_seq, target_seq)\n",
    "            \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_Loss[-1] += loss \n",
    "            else:\n",
    "                break         \n",
    "        else:\n",
    "            break\n",
    "    #save the weights of the Model        \n",
    "    if e % save_every == 0:\n",
    "        torch.save(model.state_dict(), f'./net_{e}.pth')\n",
    "    #print the Loss and Time of the training \n",
    "    if e % print_every == 0:   \n",
    "        print('Epoch {}: Total Loss = {}, Avg. Time/Epoch = {}'\n",
    "                .format(e, total_Loss[-1], (time.time() - start) / print_every))\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( net_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(total_Loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
