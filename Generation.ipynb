{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_dim, batch_size, hidden_dim, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embeddings = nn.Embedding(input_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        hidden = self.init_hidden()\n",
    "        \n",
    "        embeds = self.embeddings(inputs)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds.view(len(net_in_tensor), 1, -1), hidden)\n",
    "        \n",
    "        prediction = self.linear(lstm_out.view(len(net_in_tensor), -1))\n",
    "        pre_scores = F.log_softmax(prediction, dim=1)\n",
    "        return pre_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embeddings): Embedding(72, 30)\n",
       "  (lstm): LSTM(30, 36)\n",
       "  (linear): Linear(in_features=36, out_features=72, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/notes', 'rb') as filepath:\n",
    "    notes = pickle.load(filepath)\n",
    "\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "vocab_size = len(note_to_int)\n",
    "\n",
    "seq_len = 100 \n",
    "\n",
    "input_size = vocab_size\n",
    "\n",
    "embedding_dim = 30\n",
    "batch_size = 1\n",
    "hidden_dim = 36\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = LSTM(input_size, embedding_dim, batch_size, hidden_dim, vocab_size)\n",
    "\n",
    "model.load_state_dict(torch.load('./net_50.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, trans):\n",
    "    idxs = [trans[w] for w in seq]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(int_seq):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in int_seq:\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.5\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp='test_output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split = torch.randint(0, vocab_size, (1, 100))\n",
    "split = np.random.randint(0, len(notes)-100) \n",
    "start = notes[split : split + 100]\n",
    "net_in_tensor =  torch.tensor(prepare_sequence(start, note_to_int), dtype=torch.long)\n",
    "prediction_out = torch.tensor([0])\n",
    "# generate 500 notes\n",
    "for note_index in range(500):\n",
    "\n",
    "    prediction = model(net_in_tensor)\n",
    "\n",
    "    feed_note = torch.tensor([torch.argmax(torch.narrow(prediction, 0, 99, 1))])\n",
    "\n",
    "    net_in_tensor = torch.narrow(net_in_tensor, 0, 1, 99)\n",
    "\n",
    "    net_in_tensor = torch.cat((net_in_tensor, feed_note), 0)\n",
    "\n",
    "    prediction_out = torch.cat((prediction_out, feed_note), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [0, 36, 55, 43, 44, 43, 44, 43, 44, 57, 44, 43, 44, 43, 44, 43, 23, 31,\n",
    "        64, 64, 32, 44,  0, 36, 25, 55, 55, 64, 36, 55,  5, 42, 32, 30,  5, 57,\n",
    "        32, 36, 11, 36, 33, 36, 33, 56, 53, 49, 30, 44, 43, 23, 67, 36, 45, 57,\n",
    "        36, 55, 43, 23, 21, 30, 45, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generatednotes = prepare_sequence(a, int_to_note)\n",
    "\n",
    "create_midi(Generatednotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
